# 📚 RAG-Based Application with Groq LLM

This project is a **Retrieval-Augmented Generation (RAG) application** built with **Groq LLM** and **Streamlit**.  
The app allows users to upload documents (e.g., PDFs), automatically vectorize them, and then query the content using natural language.  
By combining a **vector database** with **Groq’s fast inference**, the application provides accurate, context-aware responses in real time.  

---

## 🚀 Features
- 📄 **Document Upload**: Supports PDFs and text documents.  
- 🔎 **RAG Pipeline**: Embeds documents into a vector database for efficient retrieval.  
- 🤖 **Groq LLM Integration**: Uses **Groq-powered LLaMA-3.1-70B** for ultra-fast inference.  
- 🧠 **Conversational Memory**: Maintains chat history for context-aware responses.  
- 🖥️ **Streamlit UI**: Simple, interactive frontend for querying and displaying answers.  

---

## 🛠️ Tech Stack
- **Python** (backend)  
- **Streamlit** (frontend)  
- **LangChain** (RAG pipeline)  
- **Chroma** (vector database)  
- **Groq API** (LLM inference)  
- **Unstructured / PyPDF** (document parsing)  
