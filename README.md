# ğŸ“š RAG-Based Application with Groq LLM

This project is a **Retrieval-Augmented Generation (RAG) application** built with **Groq LLM** and **Streamlit**.  
The app allows users to upload documents (e.g., PDFs), automatically vectorize them, and then query the content using natural language.  
By combining a **vector database** with **Groqâ€™s fast inference**, the application provides accurate, context-aware responses in real time.  

---

## ğŸš€ Features
- ğŸ“„ **Document Upload**: Supports PDFs and text documents.  
- ğŸ” **RAG Pipeline**: Embeds documents into a vector database for efficient retrieval.  
- ğŸ¤– **Groq LLM Integration**: Uses **Groq-powered LLaMA-3.1-70B** for ultra-fast inference.  
- ğŸ§  **Conversational Memory**: Maintains chat history for context-aware responses.  
- ğŸ–¥ï¸ **Streamlit UI**: Simple, interactive frontend for querying and displaying answers.  

---

## ğŸ› ï¸ Tech Stack
- **Python** (backend)  
- **Streamlit** (frontend)  
- **LangChain** (RAG pipeline)  
- **Chroma** (vector database)  
- **Groq API** (LLM inference)  
- **Unstructured / PyPDF** (document parsing)  
